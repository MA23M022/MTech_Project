{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import hdbscan\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stl import mesh as np_mesh  # Import numpy-stl\n",
    "from scipy.spatial.distance import cdist\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import gc\n",
    "import psutil\n",
    "import shutil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proj_DBhelper:\n",
    "    def __init__(self, data_base):\n",
    "        try:\n",
    "            self.conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"\", database=data_base)\n",
    "            self.mycursor = self.conn.cursor()\n",
    "        except:\n",
    "            print(\"some has occured\")\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            print(\"Database is connected\")\n",
    "\n",
    "    def create_table(self, table_name):\n",
    "        try:\n",
    "            create_table_query = f\"\"\"\n",
    "                                    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                                        id INT(11) AUTO_INCREMENT PRIMARY KEY,\n",
    "                                        file_name VARCHAR(255) NULL,\n",
    "                                        proj_feature_vector VARCHAR(255) NULL,\n",
    "                                        cluster_label INT(11) NULL\n",
    "                                    );\"\"\"\n",
    "            self.mycursor.execute(create_table_query)\n",
    "            self.conn.commit()\n",
    "        except:\n",
    "            print(\"Table has not been created\")\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            print(\"Table is ready.\")\n",
    "\n",
    "    def register(self, stl_file, cluster_label = -1):\n",
    "        try:\n",
    "            insert_query = \"\"\"INSERT INTO `stl_5000` (`file_name`, `cluster_label`) VALUES (%s, %s);\"\"\"\n",
    "            self.mycursor.execute(insert_query, (stl_file, cluster_label))\n",
    "            self.conn.commit()\n",
    "        except:\n",
    "            print(f\"{stl_file} could not entered in the database\")\n",
    "        else:\n",
    "            print(f\"{stl_file} is entered in the database\")\n",
    "        \n",
    "        \n",
    "    def update_cluster_label(self, stl_file, cluster_label):\n",
    "        try:\n",
    "            update_query = \"\"\"UPDATE `stl_5000` SET `cluster_label` = %s WHERE `stl_5000`.`file_name` = %s;\"\"\"\n",
    "            self.mycursor.execute(update_query, (cluster_label, stl_file))\n",
    "            self.conn.commit()\n",
    "        except:\n",
    "            print(f\"could not able to update for the {stl_file}\")\n",
    "        else:\n",
    "            print(f\"updatation of label is sucessfull for the {stl_file}\")\n",
    "        \n",
    "        \n",
    "    def update_feature_vector(self, stl_file, proj_feature):\n",
    "        try:\n",
    "            update_query = \"\"\"UPDATE `stl_5000` SET `proj_feature_vector` = %s WHERE `stl_5000`.`file_name` = %s;\"\"\"\n",
    "            self.mycursor.execute(update_query, (proj_feature, stl_file))\n",
    "            self.conn.commit()\n",
    "        except:\n",
    "            print(f\"The vectors could not enter in the database\")\n",
    "        else:\n",
    "            print(f\"The vectors are entered in the database\")\n",
    "        \n",
    "    def search_feature_vector(self, stl_file):\n",
    "        try:\n",
    "            self.mycursor.execute(\"\"\"SELECT proj_feature_vector FROM stl_5000\n",
    "                                WHERE file_name LIKE '{}'\"\"\".format(stl_file))\n",
    "            data = self.mycursor.fetchone()\n",
    "            return data[0]\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_feature_vector_through_id(self, id_number):\n",
    "        try:\n",
    "            self.mycursor.execute(\"\"\"SELECT file_name, proj_feature_vector FROM stl_5000\n",
    "                                WHERE id LIKE '{}'\"\"\".format(id_number))\n",
    "            data = self.mycursor.fetchone()\n",
    "            return data[0], data[1]\n",
    "        except:\n",
    "            return None, None\n",
    "        \n",
    "    def search_cluster_label_files(self, cluster_label):\n",
    "        try:\n",
    "            search_query = \"\"\"SELECT `file_name` FROM `stl_5000` WHERE `cluster_label` LIKE %s\"\"\"\n",
    "            self.mycursor.execute(search_query, (cluster_label,))\n",
    "            data = self.mycursor.fetchall()\n",
    "            return data, True\n",
    "        except:\n",
    "            return None, False\n",
    "    \n",
    "    def search_corresponding_label(self, stl_file):\n",
    "        try:\n",
    "            search_query = \"\"\"SELECT `cluster_label` FROM `stl_5000` WHERE `file_name` LIKE %s\"\"\"\n",
    "            self.mycursor.execute(search_query, (stl_file,))\n",
    "            data = self.mycursor.fetchone()\n",
    "            return data[0]\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def search_max_cluster_label(self):\n",
    "        try:\n",
    "            query = \"\"\"SELECT MAX(cluster_label) FROM stl_5000\"\"\"\n",
    "            self.mycursor.execute(query)\n",
    "            max_value = self.mycursor.fetchone()\n",
    "            return max_value[0]\n",
    "        except:\n",
    "            return -1\n",
    "        \n",
    "    def search_total_files(self):\n",
    "        try:\n",
    "            query = \"\"\"SELECT MAX(id) FROM stl_5000\"\"\"\n",
    "            self.mycursor.execute(query)\n",
    "            max_value = self.mycursor.fetchone()\n",
    "            return max_value[0]\n",
    "        except:\n",
    "            return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proj_Feature_Data_Base:\n",
    "    def __init__(self, data_base):                               \n",
    "        self.db = Proj_DBhelper(data_base)   #connect to the database\n",
    "\n",
    "    def table_creation(self, table_name):\n",
    "        self.db.create_table(table_name)    #create table\n",
    "        \n",
    "    def send_files_to_db(self, stl_files):\n",
    "        for file in stl_files:\n",
    "            self.db.register(file)\n",
    "                \n",
    "    def send_labels_to_db(self, stl_file, cluster_label):\n",
    "        self.db.update_cluster_label(stl_file, cluster_label)\n",
    "\n",
    "    def send_feature_vector_to_db(self, stl_file, proj_vector):\n",
    "        self.db.update_feature_vector(stl_file, proj_vector)\n",
    "        \n",
    "    def find_feature_vector(self, stl_file):\n",
    "        vector_file_name = self.db.search_feature_vector(stl_file)\n",
    "        if vector_file_name:\n",
    "            return vector_file_name, True\n",
    "        else:\n",
    "            return None, False\n",
    "        \n",
    "    def find_feature_vector_through_id(self, id_number):\n",
    "        file_name, vector_file_name = self.db.search_feature_vector_through_id(id_number)\n",
    "        if vector_file_name:\n",
    "            return file_name, vector_file_name, True\n",
    "        else:\n",
    "            return file_name, None, False\n",
    "        \n",
    "    def find_files(self, cluster_label):\n",
    "        data, flag = self.db.search_cluster_label_files(cluster_label)\n",
    "        if(flag):\n",
    "            if(len(data) > 0):\n",
    "                return data\n",
    "        return None\n",
    "\n",
    "    def find_label(self, stl_file):\n",
    "        data = self.db.search_corresponding_label(stl_file)\n",
    "        if data:\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def maximum_cluster_label(self):\n",
    "        number = self.db.search_max_cluster_label()\n",
    "        return number\n",
    "    \n",
    "    def count_files(self):\n",
    "        number = self.db.search_total_files()\n",
    "        return number\n",
    "\n",
    "    def data_distribution(self):\n",
    "        length = self.maximum_cluster_label()\n",
    "        dictionary = {}\n",
    "        for i in range(length+1):\n",
    "            num_files, flag = self.db.search_cluster_label_files(i)\n",
    "            if(flag):\n",
    "                if(len(num_files) > 0):\n",
    "                    dictionary[i] = len(num_files)\n",
    "            else:\n",
    "                dictionary[i] = 0\n",
    "            \n",
    "\n",
    "        clusters = list(dictionary.keys())\n",
    "        values = list(dictionary.values())\n",
    "        \n",
    "        fig = plt.figure(figsize = (20, 10))\n",
    "\n",
    "        # creating the bar plot\n",
    "        plt.bar(clusters, values, color ='maroon', width = 0.8)\n",
    "\n",
    "        plt.xlabel(\"Cluster labels\")\n",
    "        plt.ylabel(\"No. of files of corresponding labels\")\n",
    "        plt.title(\"clusters vs no. of files\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database is connected\n"
     ]
    }
   ],
   "source": [
    "s1 = Proj_Feature_Data_Base(\"aws_database_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculate_clusters:\n",
    "    def __init__(self, feature_data_base, cluster_representative_folder,\n",
    "                  min_cluster_size=2, min_samples=1, similarity_threshold=0.97):\n",
    "        self.feature_data_base = feature_data_base\n",
    "        self.cluster_representative_folder = cluster_representative_folder\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.min_samples = min_samples\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.stl_files = [] \n",
    "        self.cluster_minus_one_folder = []\n",
    "        self.num_stl_files = None\n",
    "        self.cluster_labels = None\n",
    "        self.max_cluster_label = None\n",
    "        self.stl_label_feature_dict = defaultdict(dict)\n",
    "        self.representative_dict = defaultdict(lambda: np.array([]))\n",
    "        self.all_features = []\n",
    "        self.error_files = []\n",
    "\n",
    "    #### ------------------------------------------------------------ cluster generation ------------------------------------------------------------------####\n",
    "    def extract_feature_from_database(self):\n",
    "        self.num_stl_files = self.feature_data_base.count_files()\n",
    "        for id_number in range(1, self.num_stl_files+1):                    # Start enumeration at 1\n",
    "            stl_file, feature_file_name, flag = self.feature_data_base.find_feature_vector_through_id(id_number)\n",
    "            if flag:\n",
    "                feature_file_name = feature_file_name.replace(\"\\\\\", \"/\")\n",
    "                if os.path.exists(feature_file_name):\n",
    "                    print(f\"The directory is : {feature_file_name}\")\n",
    "                    feature = np.load(feature_file_name)\n",
    "                    feature = np.array(feature)\n",
    "                    self.all_features.append(feature)\n",
    "                    self.stl_files.append(stl_file)\n",
    "                    print(f\"Fetched feature for {id_number} : {stl_file}\")\n",
    "                else:\n",
    "                    self.error_files.append(stl_file)\n",
    "                    print(f\"The directory does not exist for {id_number}\")\n",
    "            else:\n",
    "                if stl_file:\n",
    "                    self.error_files.append(stl_file)\n",
    "                    print(f\"Facing error to fetch feature of for {id_number} :  {stl_file}\")\n",
    "                else:\n",
    "                    print(f\"The ID Number {id_number} does not exist in the data table\")\n",
    "\n",
    "        for file in self.error_files:\n",
    "            self.feature_data_base.send_labels_to_db(file, -2)\n",
    "\n",
    "    def generate_clusters(self):\n",
    "        self.extract_feature_from_database()\n",
    "        all_features = np.array(self.all_features)\n",
    "        scaler = StandardScaler()\n",
    "        all_features = scaler.fit_transform(all_features)\n",
    "\n",
    "        # Perform HDBSCAN clustering\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=self.min_cluster_size, min_samples=self.min_samples)\n",
    "        self.cluster_labels = clusterer.fit_predict(all_features)\n",
    "\n",
    "    def move_files_to_clusters(self):      # Move files to their respective cluster folders\n",
    "\n",
    "        for idx, label in enumerate(self.cluster_labels):\n",
    "            label = int(label)\n",
    "            self.stl_label_feature_dict[label][self.stl_files[idx]] = self.all_features[idx]\n",
    "\n",
    "        self.max_cluster_label = int(np.max(self.cluster_labels))\n",
    "\n",
    "\n",
    "    ###-----------------------------------------Preprocess and representative calculation-----------------------------------------------------###\n",
    "\n",
    "    # Function to normalize a feature vector\n",
    "    def normalize_vector(self, vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        return vector / norm if norm > 0 else vector\n",
    "    \n",
    "\n",
    "    def calculate_medoid(self, cluster_points):\n",
    "        distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        medoid_index = np.argmin(distances.sum(axis=1))\n",
    "        return cluster_points[medoid_index]\n",
    "    \n",
    "        \n",
    "    # Function to process files in a cluster\n",
    "    def process_cluster_and_calculate_representative(self, cluster_label):\n",
    "        cluster_files = list(self.stl_label_feature_dict[cluster_label].keys())\n",
    "        cluster_features = []\n",
    "        file_paths = []\n",
    "\n",
    "        # Load features\n",
    "        for file in cluster_files:\n",
    "            feature = self.stl_label_feature_dict[cluster_label][file]      #self.feature_data_base.find_feature_vector(file)\n",
    "            normalized_feature = self.normalize_vector(feature)\n",
    "            cluster_features.append(normalized_feature)\n",
    "            file_paths.append(file)\n",
    "\n",
    "        if not cluster_features:\n",
    "            return []\n",
    "\n",
    "        cluster_features = np.array(cluster_features)\n",
    "        medoid = self.calculate_medoid(cluster_features)\n",
    "        files_to_move = set()\n",
    "        for i, feature in enumerate(cluster_features):\n",
    "            similarity_to_centroid = cosine_similarity([feature], [medoid])[0][0]\n",
    "            if similarity_to_centroid < self.similarity_threshold:\n",
    "                files_to_move.add(file_paths[i])                     # dissimilar files.\n",
    "\n",
    "        refined_cluster_files = []\n",
    "        for file in cluster_files:\n",
    "            if file in files_to_move:\n",
    "                continue\n",
    "            self.feature_data_base.send_labels_to_db(file, cluster_label)\n",
    "            refined_cluster_files.append(file)                       # refined the files which are in the cluster label.\n",
    "\n",
    "        refined_cluster_features = []\n",
    "        for file in refined_cluster_files:\n",
    "            feature = self.stl_label_feature_dict[cluster_label][file]\n",
    "            refined_cluster_features.append(feature)                  # Features of the files which are in the cluster label.\n",
    "\n",
    "        if len(refined_cluster_features) >= 1:\n",
    "            refined_cluster_features = np.array(refined_cluster_features)\n",
    "            cluster_medoid = self.calculate_medoid(refined_cluster_features)\n",
    "            self.representative_dict[cluster_label] = cluster_medoid\n",
    "            print(f\"Medoid is calculated for the cluster_{cluster_label}\")    #calculate the representative of the cluster label.\n",
    "        else:\n",
    "            print(f\"failed to calculate medoid for cluster_{cluster_label}\")\n",
    "\n",
    "        return list(files_to_move)\n",
    "    \n",
    "    # Function to move files to single cluster\n",
    "    def move_files_to_single_cluster(self, cluster_label, files_to_move):\n",
    "        for file in files_to_move:\n",
    "            self.max_cluster_label += 1\n",
    "            self.feature_data_base.send_labels_to_db(file, self.max_cluster_label)\n",
    "            self.representative_dict[self.max_cluster_label] = self.stl_label_feature_dict[cluster_label][file]\n",
    "            print(f\"Medoid is calculated for the cluster_{self.max_cluster_label}\")\n",
    "\n",
    "    # Iterate through each cluster and process files\n",
    "    def process_cluster_and_move_dissimilar_files(self):\n",
    "        unique_cluster_labels = self.max_cluster_label\n",
    "        for cluster_label in range(unique_cluster_labels+1):\n",
    "            if cluster_label != -1:\n",
    "                files_to_move = self.process_cluster_and_calculate_representative(cluster_label)\n",
    "                self.move_files_to_single_cluster(cluster_label, files_to_move)\n",
    "\n",
    "\n",
    "    ####------------------------------------------------------ new clusters ----------------------------------------------------------------####\n",
    "    \n",
    "    def create_new_clusters_for_cluster_minus_one(self):\n",
    "        files_in_cluster_minus_one = list(self.stl_label_feature_dict[-1].keys())\n",
    "        for file in files_in_cluster_minus_one:\n",
    "            self.max_cluster_label += 1\n",
    "            self.feature_data_base.send_labels_to_db(file, self.max_cluster_label)\n",
    "            self.representative_dict[self.max_cluster_label] = self.stl_label_feature_dict[-1][file]\n",
    "            print(f\"Medoid is calculated for the cluster_{self.max_cluster_label}\")\n",
    "\n",
    "    ####------------------------------------------------------ Save cluster Representative ------------------------------------------------####\n",
    "\n",
    "    def create_cluster_representative_folder(self):\n",
    "        files_in_particular_cluster = list(self.stl_label_feature_dict[0].keys())\n",
    "        particular_feature_vector = self.stl_label_feature_dict[0][files_in_particular_cluster[0]]\n",
    "        vector_length = len(particular_feature_vector)\n",
    "        print(vector_length)\n",
    "        cluster_representatives = np.zeros((self.max_cluster_label+1, vector_length))\n",
    "        for i in range(self.max_cluster_label+1):\n",
    "            cluster_representatives[i] = self.representative_dict[i]\n",
    "\n",
    "        #->->->-> put the all representative 2d vector in the s3 bucket.\n",
    "        all_clusters_representative_file_name = os.path.join(self.cluster_representative_folder, f'all_clusters_representative.npy')\n",
    "        np.save(all_clusters_representative_file_name, cluster_representatives)\n",
    "        print(f\"Complete!\")\n",
    "\n",
    "\n",
    "    ###------------------------------------------------------- Calculate metric -------------------------------------------------------------###\n",
    "\n",
    "    def load_features_and_labels(self):\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        max_cluster_label = self.feature_data_base.maximum_cluster_label()\n",
    "        for cluster_label in range(max_cluster_label+1):\n",
    "            cluster_files_from_db = self.feature_data_base.find_files(cluster_label)\n",
    "            cluster_files = []\n",
    "            if cluster_files_from_db != None:\n",
    "                for element in cluster_files_from_db:\n",
    "                    cluster_files.append(element[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for file in cluster_files:\n",
    "                feature, flag = self.feature_data_base.find_feature_vector(file)\n",
    "                if flag:\n",
    "                    all_features.append(feature)\n",
    "                    all_labels.append(cluster_label)\n",
    "                else:\n",
    "                    print(f\"Feature file not found for {file}\")\n",
    "\n",
    "        return np.array(all_features), np.array(all_labels)\n",
    "    \n",
    "    def calculate_metrices(self):\n",
    "        features, labels = self.load_features_and_labels()\n",
    "        # Compute the Silhouette Score for all samples\n",
    "        if len(features) > 1 and len(set(labels)) > 1:\n",
    "            silhouette_avg = silhouette_score(features, labels)\n",
    "            calinski_harabasz_avg = calinski_harabasz_score(features, labels)\n",
    "            davies_bouldin_avg = davies_bouldin_score(features, labels)\n",
    "            print(f\"Overall Silhouette Score : {silhouette_avg:.4f}\")\n",
    "            print(f\"Overall calinski harabasz avg : {calinski_harabasz_avg:.4f}\")\n",
    "            print(f\"Overall davies bouldin score avg : {davies_bouldin_avg:.4f}\")\n",
    "        else:\n",
    "            print(\"Not enough data or clusters to compute Silhouette Score.\")\n",
    "        print(\"Processing complete.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory is : C:\\Users\\soura\\Desktop\\PDSV_codes\\stl_5000_representative\n"
     ]
    }
   ],
   "source": [
    "cluster_representative_folder = r\"C:\\Users\\soura\\Desktop\\PDSV_codes\\stl_5000_representative\"\n",
    "os.makedirs(cluster_representative_folder, exist_ok=True)\n",
    "if os.path.exists(cluster_representative_folder):\n",
    "    print(f\"The directory is : {cluster_representative_folder}\")\n",
    "else:\n",
    "    print(f\"The directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance = calculate_clusters(s1, cluster_representative_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate_clusters_instance.extract_feature_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['173829_105-8395_prt.stl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate_clusters_instance.error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.generate_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.move_files_to_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_clusters_instance.max_cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.process_cluster_and_move_dissimilar_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1487"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_clusters_instance.max_cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.create_new_clusters_for_cluster_minus_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_clusters_instance.max_cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "calculate_clusters_instance.create_cluster_representative_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_all_representative_path = os.path.join(cluster_representative_folder, f'all_clusters_representative.npy')\n",
    "vectors1 = None\n",
    "if os.path.exists(aws_all_representative_path):\n",
    "    a_representative = np.load(aws_all_representative_path)\n",
    "    vectors1 = np.array(a_representative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2814, 5120)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_labels = calculate_clusters_instance.cluster_labels\n",
    "print(type(c_labels))\n",
    "print(c_labels.shape)\n",
    "print(c_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_files = calculate_clusters_instance.stl_files\n",
    "print(type(s_files))\n",
    "print(len(s_files))\n",
    "print(s_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(5120,)\n",
      "[0.        0.        0.8500222 ... 0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "stl_all_features = calculate_clusters_instance.all_features\n",
    "print(type(stl_all_features))\n",
    "print(stl_all_features[0].shape)\n",
    "print(stl_all_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.all_features = stl_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.stl_files = s_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_clusters_instance.cluster_labels = c_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.maximum_cluster_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.data_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = s1.maximum_cluster_label()\n",
    "dictionary = {}\n",
    "for i in range(length+1):\n",
    "    num_files, flag = s1.db.search_cluster_label_files(i)\n",
    "    if(flag):\n",
    "        if(len(num_files) > 0):\n",
    "            dictionary[i] = len(num_files)\n",
    "    else:\n",
    "        dictionary[i] = 0\n",
    "    \n",
    "\n",
    "clusters = list(dictionary.keys())\n",
    "values = list(dictionary.values())\n",
    "\n",
    "fig = plt.figure(figsize = (20, 10))\n",
    "\n",
    "# creating the bar plot\n",
    "plt.bar(clusters[:1500], values[:1500], color ='maroon', width = 0.9)\n",
    "\n",
    "plt.xlabel(\"Cluster labels\")\n",
    "plt.ylabel(\"No. of files of corresponding labels\")\n",
    "plt.title(\"clusters vs no. of files\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_6_files = []\n",
    "for i, ele in enumerate(values):\n",
    "    if ele == 6:\n",
    "        cluster_6_files.append(i)\n",
    "\n",
    "print(cluster_6_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdsv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
